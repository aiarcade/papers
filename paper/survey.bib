@article{Wu2015,
author = {Wu, Ren and Yan, Shengen and Shan, Yi and Dang, Qingqing and Sun, Gang},
file = {:home/mahesh/paper/survey/ref/1501.02876v1.pdf:pdf},
journal = {arXiv Prepr. arXiv1501.02876},
mendeley-groups = {survey\_icip2015},
title = {{Deep Image: Scaling up Image Recognition}},
url = {http://arxiv.org/abs/1501.02876},
year = {2015}
}
@article{Krizhevsky2014,
abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales signiﬁcantly better than all alternatives when applied to modern convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.5997v2},
author = {Krizhevsky, Alex},
eprint = {arXiv:1404.5997v2},
file = {:home/mahesh/paper/survey/ref/1404.5997v2.pdf:pdf},
mendeley-groups = {survey\_icip2015},
pages = {1--7},
title = {{One weird trick for parallelizing convolutional neural networks}},
url = {http://arxiv.org/abs/1404.5997},
year = {2014}
}

@article{Yadan2013,
abstract = {In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs within the same server.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.5853v4},
author = {Yadan, O and Adams, Keith},
eprint = {arXiv:1312.5853v4},
file = {:home/mahesh/paper/survey/ref/1312.5853v4.pdf:pdf},
journal = {arXiv},
mendeley-groups = {survey\_icip2015},
pages = {10--13},
title = {{Multi-GPU Training of ConvNets}},
url = {http://www.nvidia.com/content/tesla/pdf/machine-learning/multi-gpu-training-convnets.pdf},
year = {2013}
}

@article{Krizhevsky2014,
abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales signiﬁcantly better than all alternatives when applied to modern convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.5997v2},
author = {Krizhevsky, Alex},
eprint = {arXiv:1404.5997v2},
file = {:home/mahesh/paper/survey/ref/1404.5997v2.pdf:pdf},
mendeley-groups = {survey\_icip2015},
pages = {1--7},
title = {{One weird trick for parallelizing convolutional neural networks}},
url = {http://arxiv.org/abs/1404.5997},
year = {2014}
}
@article{Berg2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.0575v2},
author = {Berg, Alex and Deng, J},
eprint = {arXiv:1409.0575v2},
file = {:home/mahesh/paper/survey/ref/1409.0575v2.pdf:pdf},
journal = {Challenge},
keywords = {benchmark,dataset,large-scale,object detection,object recognition},
mendeley-groups = {survey\_icip2015},
title = {{Imagenet large scale visual recognition challenge 2010}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Large+Scale+Visual+Recognition+Challenge+2010\#2},
year = {2010}
}
@article{Arge2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v5},
author = {Karen Simonyan and Andrew Zisserman},
eprint = {arXiv:1409.1556v5},
file = {:data/dev/object\_classifiers/ref/6.pdf:pdf},
mendeley-groups = {survey\_icip2015},
pages = {1--13},
title = {{V d c n l -s i r}},
year = {2015}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
author = {Nair, Vinod and Hinton, Geoffrey E},
file = {:home/mahesh/paper/survey/ref/reluICML.pdf:pdf},
isbn = {9781605589077},
journal = {Proc. 27th Int. Conf. Mach. Learn.},
mendeley-groups = {survey\_icip2015},
number = {3},
pages = {807--814},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation
algorithm constitute the best example of a successful gradient based
learning technique. Given an appropriate network architecture,
gradient-based learning algorithms can be used to synthesize a complex
decision surface that can classify high-dimensional patterns, such as
handwritten characters, with minimal preprocessing. This paper reviews
various methods applied to handwritten character recognition and
compares them on a standard handwritten digit recognition task.
Convolutional neural networks, which are specifically designed to deal
with the variability of 2D shapes, are shown to outperform all other
techniques. Real-life document recognition systems are composed of
multiple modules including field extraction, segmentation recognition,
and language modeling. A new learning paradigm, called graph transformer
networks (GTN), allows such multimodule systems to be trained globally
using gradient-based methods so as to minimize an overall performance
measure. Two systems for online handwriting recognition are described.
Experiments demonstrate the advantage of global training, and the
flexibility of graph transformer networks. A graph transformer network
for reading a bank cheque is also described. It uses convolutional
neural network character recognizers combined with global training
techniques to provide record accuracy on business and personal cheques.
It is deployed commercially and reads several million cheques per day
},
author = {LeCun, Yann and Bottou, L\'{e}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
file = {:home/mahesh/paper/survey/ref/lecun-01a.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proc. IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
mendeley-groups = {survey\_icip2015},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Szegedy,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.4842v1},
author = {Szegedy, Christian and Reed, Scott and Sermanet, Pierre and Vanhoucke, Vincent and Rabinovich, Andrew},
eprint = {arXiv:1409.4842v1},
file = {:home/mahesh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - Unknown - Going deeper with convolutions.pdf:pdf},
mendeley-groups = {survey\_icip2015,survey\_icip2015/2014},
pages = {1--12},
title = {{Going deeper with convolutions}}
}

@article{Zeiler2013,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D and Fergus, Rob},
eprint = {1311.2901},
file = {:data/dev/object\_classifiers/ref/zeilerECCV2014.pdf:pdf},
journal = {arXiv preprint arXiv:1311.2901},
mendeley-groups = {survey\_icip2015/2013},
pages = {818--833},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {http://arxiv.org/abs/1311.2901},
year = {2013}
}

@article{Krizhevsky2012a,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
eprint = {1102.0183},
file = {:data/dev/object\_classifiers/ref/7.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
mendeley-groups = {survey\_icip2015,survey\_icip2015/2012},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}


@article{Sermanet2013,
abstract = {We present an integrated framework for using ConvolutionalNetworks for classi- fication, localization and detection.We showhowamultiscale and slidingwindow approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object bound- aries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simul- taneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNetLarge ScaleVisual RecognitionChallenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competitionwork, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
file = {:home/mahesh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sermanet et al. - 2013 - OverFeat Integrated Recognition , Localization and Detection using Convolutional Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6229},
mendeley-groups = {survey\_icip2015},
pages = {1--15},
title = {{OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@article{He2014,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224x224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170x faster than the recent leading method R-CNN (and 24-64x faster overall), while achieving better or comparable accuracy on Pascal VOC 2007.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/TPAMI.2015.2389824},
eprint = {1406.4729},
file = {:data/dev/object\_classifiers/ref/1406.4729v3.pdf:pdf},
journal = {arXiv preprint arXiv \ldots},
mendeley-groups = {survey\_icip2015,survey\_icip2015/2014},
pages = {1--14},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
url = {http://arxiv.org/abs/1406.4729v1$\backslash$npapers3://publication/uuid/09415D06-A785-4329-82C0-B9FF2B1FEAB7},
volume = {cs.CV},
year = {2014}
}
@article{Sivic2003,
author = {Sivic, J and Zisserman, A},
file = {:data/dev/object\_classifiers/ref/sivic03.pdf:pdf},
isbn = {0769519504},
journal = {Proc. CVPR},
mendeley-groups = {survey\_icip2015/fundmental\_papers},
number = {Iccv},
pages = {2--9},
title = {{A text retrieval approach to object matching in videos}},
year = {2003}
}

@article{Grauman2005,
abstract = {Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This "pyramid match" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches},
author = {Grauman, Kristen and Darrell, Trevor},
doi = {10.1109/ICCV.2005.239},
file = {:data/dev/object\_classifiers/ref/grauman\_darrell\_iccv05.pdf:pdf},
isbn = {076952334X},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
mendeley-groups = {survey\_icip2015/fundmental\_papers},
number = {October},
pages = {1458--1465},
title = {{The pyramid match kernel: Discriminative classification with sets of image features}},
volume = {II},
year = {2005}
}

@INPROCEEDINGS{1641019, 
author={Lazebnik, S. and Schmid, C. and Ponce, J.}, 
booktitle={Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on}, 
title={Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories}, 
year={2006}, 
month={}, 
volume={2}, 
pages={2169-2178}, 
keywords={Histograms;Image databases;Image recognition;Image representation;Image segmentation;Layout;Object recognition;Robustness;Shape;Spatial databases}, 
doi={10.1109/CVPR.2006.68}, 
ISSN={1063-6919},}

@article{Zeiler2013,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D and Fergus, Rob},
eprint = {1311.2901},
file = {:data/dev/object\_classifiers/ref/zeilerECCV2014.pdf:pdf},
journal = {arXiv preprint arXiv:1311.2901},
mendeley-groups = {survey\_icip2015/2013},
pages = {818--833},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {http://arxiv.org/abs/1311.2901},
year = {2013}
}

@article{Goodfellow2013,
abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
archivePrefix = {arXiv},
arxivId = {1302.4389},
author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
eprint = {1302.4389},
file = {:data/dev/object\_classifiers/ref/1302.4389v4.pdf:pdf},
journal = {arXiv preprint},
mendeley-groups = {survey\_icip2015/2013},
pages = {1319--1327},
title = {{Maxout Networks}},
url = {http://arxiv.org/abs/1302.4389},
year = {2013}
}

@article{Lin2013,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
eprint = {1312.4400},
file = {:data/dev/object\_classifiers/ref/3.pdf:pdf},
journal = {arXiv preprint},
mendeley-groups = {survey\_icip2015,survey\_icip2015/2013},
pages = {10},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}










