@InProceedings{C2,
  author = 	 "Jones, C.D. and Smith, A.B. and Roberts, E.F.",
  title =        "Article Title",
  booktitle =        "Proceedings Title",
  organization = "IEEE",
  year = 	 "2003",
  volume = 	 "II",
  pages = 	 "803-806"
}

@Article{Lamp86,
  author =       "A.B. Smith and C.D. Jones and E.F. Roberts",
  title =        "Article Title",
  journal = 	 "Journal",
  year = 	 "1920",
  volume = 	 "62",
  pages = 	 "291-294",
  month = 	 "January"
}
@article{Wu2015,
author = {Wu, Ren and Yan, Shengen and Shan, Yi and Dang, Qingqing and Sun, Gang},
file = {:home/mahesh/paper/survey/ref/1501.02876v1.pdf:pdf},
journal = {arXiv Prepr. arXiv1501.02876},
mendeley-groups = {survey\_icip2015},
title = {{Deep Image: Scaling up Image Recognition}},
url = {http://arxiv.org/abs/1501.02876},
year = {2015}
}
@article{Krizhevsky2014,
abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales signiﬁcantly better than all alternatives when applied to modern convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.5997v2},
author = {Krizhevsky, Alex},
eprint = {arXiv:1404.5997v2},
file = {:home/mahesh/paper/survey/ref/1404.5997v2.pdf:pdf},
mendeley-groups = {survey\_icip2015},
pages = {1--7},
title = {{One weird trick for parallelizing convolutional neural networks}},
url = {http://arxiv.org/abs/1404.5997},
year = {2014}
}

@article{Yadan2013,
abstract = {In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs within the same server.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.5853v4},
author = {Yadan, O and Adams, Keith},
eprint = {arXiv:1312.5853v4},
file = {:home/mahesh/paper/survey/ref/1312.5853v4.pdf:pdf},
journal = {arXiv},
mendeley-groups = {survey\_icip2015},
pages = {10--13},
title = {{Multi-GPU Training of ConvNets}},
url = {http://www.nvidia.com/content/tesla/pdf/machine-learning/multi-gpu-training-convnets.pdf},
year = {2013}
}

@article{Krizhevsky2014,
abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales signiﬁcantly better than all alternatives when applied to modern convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.5997v2},
author = {Krizhevsky, Alex},
eprint = {arXiv:1404.5997v2},
file = {:home/mahesh/paper/survey/ref/1404.5997v2.pdf:pdf},
mendeley-groups = {survey\_icip2015},
pages = {1--7},
title = {{One weird trick for parallelizing convolutional neural networks}},
url = {http://arxiv.org/abs/1404.5997},
year = {2014}
}



