@article{Wu2015,
author = {Wu, Ren and Yan, Shengen and Shan, Yi and Dang, Qingqing and Sun, Gang},
file = {:home/mahesh/paper/survey/ref/1501.02876v1.pdf:pdf},
journal = {arXiv Prepr. arXiv1501.02876},
mendeley-groups = {survey\_icip2015},
title = {{Deep Image: Scaling up Image Recognition}},
url = {http://arxiv.org/abs/1501.02876},
year = {2015}
}
@article{Krizhevsky2014,
abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales signiﬁcantly better than all alternatives when applied to modern convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.5997v2},
author = {Krizhevsky, Alex},
eprint = {arXiv:1404.5997v2},
file = {:home/mahesh/paper/survey/ref/1404.5997v2.pdf:pdf},
mendeley-groups = {survey\_icip2015},
pages = {1--7},
title = {{One weird trick for parallelizing convolutional neural networks}},
url = {http://arxiv.org/abs/1404.5997},
year = {2014}
}

@article{Yadan2013,
abstract = {In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs within the same server.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.5853v4},
author = {Yadan, O and Adams, Keith},
eprint = {arXiv:1312.5853v4},
file = {:home/mahesh/paper/survey/ref/1312.5853v4.pdf:pdf},
journal = {arXiv},
mendeley-groups = {survey\_icip2015},
pages = {10--13},
title = {{Multi-GPU Training of ConvNets}},
url = {http://www.nvidia.com/content/tesla/pdf/machine-learning/multi-gpu-training-convnets.pdf},
year = {2013}
}

@article{Krizhevsky2014,
abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales signiﬁcantly better than all alternatives when applied to modern convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.5997v2},
author = {Krizhevsky, Alex},
eprint = {arXiv:1404.5997v2},
file = {:home/mahesh/paper/survey/ref/1404.5997v2.pdf:pdf},
mendeley-groups = {survey\_icip2015},
pages = {1--7},
title = {{One weird trick for parallelizing convolutional neural networks}},
url = {http://arxiv.org/abs/1404.5997},
year = {2014}
}
@article{Berg2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.0575v2},
author = {Berg, Alex and Deng, J},
eprint = {arXiv:1409.0575v2},
file = {:home/mahesh/paper/survey/ref/1409.0575v2.pdf:pdf},
journal = {Challenge},
keywords = {benchmark,dataset,large-scale,object detection,object recognition},
mendeley-groups = {survey\_icip2015},
title = {{Imagenet large scale visual recognition challenge 2010}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Large+Scale+Visual+Recognition+Challenge+2010\#2},
year = {2010}
}
@article{Arge2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v5},
author = {Karen Simonyan and Andrew Zisserman},
eprint = {arXiv:1409.1556v5},
file = {:data/dev/object\_classifiers/ref/6.pdf:pdf},
mendeley-groups = {survey\_icip2015},
pages = {1--13},
title = {{V d c n l -s i r}},
year = {2015}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
author = {Nair, Vinod and Hinton, Geoffrey E},
file = {:home/mahesh/paper/survey/ref/reluICML.pdf:pdf},
isbn = {9781605589077},
journal = {Proc. 27th Int. Conf. Mach. Learn.},
mendeley-groups = {survey\_icip2015},
number = {3},
pages = {807--814},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation
algorithm constitute the best example of a successful gradient based
learning technique. Given an appropriate network architecture,
gradient-based learning algorithms can be used to synthesize a complex
decision surface that can classify high-dimensional patterns, such as
handwritten characters, with minimal preprocessing. This paper reviews
various methods applied to handwritten character recognition and
compares them on a standard handwritten digit recognition task.
Convolutional neural networks, which are specifically designed to deal
with the variability of 2D shapes, are shown to outperform all other
techniques. Real-life document recognition systems are composed of
multiple modules including field extraction, segmentation recognition,
and language modeling. A new learning paradigm, called graph transformer
networks (GTN), allows such multimodule systems to be trained globally
using gradient-based methods so as to minimize an overall performance
measure. Two systems for online handwriting recognition are described.
Experiments demonstrate the advantage of global training, and the
flexibility of graph transformer networks. A graph transformer network
for reading a bank cheque is also described. It uses convolutional
neural network character recognizers combined with global training
techniques to provide record accuracy on business and personal cheques.
It is deployed commercially and reads several million cheques per day
},
author = {LeCun, Yann and Bottou, L\'{e}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
file = {:home/mahesh/paper/survey/ref/lecun-01a.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proc. IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
mendeley-groups = {survey\_icip2015},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}



